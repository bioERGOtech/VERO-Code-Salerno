{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca26cef9",
   "metadata": {},
   "source": [
    "# Data Quality Pipeline\n",
    "\n",
    "\n",
    "This notebook performs:\n",
    "- Data loading\n",
    "- Profiling (types, missingness, uniques, numeric summaries)\n",
    "- Plausibility rules (dates, age, BMI, generic bounds)\n",
    "- Outlier governance (IQR-based flags)\n",
    "- Imputation using your *variable action plan* ('Impute', 'Drop', 'Flag', etc.)\n",
    "- Exports: cleaned data, reports, and logs.\n",
    "\n",
    "> Note: Columns with more than 40% missing were removed upstream, as specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63de2bba",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\HP\\\\OneDrive\\\\Desktop\\\\VERO_code\\\\Phase_1\\\\outputs\\\\recommendations\\\\variable_action_plan.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 14\u001B[0m\n\u001B[1;32m     11\u001B[0m OUT_DIR\u001B[38;5;241m.\u001B[39mmkdir(parents\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# --- Load data & action plan ---\u001B[39;00m\n\u001B[0;32m---> 14\u001B[0m plan_all \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_excel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mPLAN_PATH\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msheet_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maction_plan\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m plan_all:\n\u001B[1;32m     16\u001B[0m     action_plan_df \u001B[38;5;241m=\u001B[39m plan_all[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maction_plan\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mcopy()\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/pandas/io/excel/_base.py:495\u001B[0m, in \u001B[0;36mread_excel\u001B[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001B[0m\n\u001B[1;32m    493\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(io, ExcelFile):\n\u001B[1;32m    494\u001B[0m     should_close \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m--> 495\u001B[0m     io \u001B[38;5;241m=\u001B[39m \u001B[43mExcelFile\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    496\u001B[0m \u001B[43m        \u001B[49m\u001B[43mio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    497\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    498\u001B[0m \u001B[43m        \u001B[49m\u001B[43mengine\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    499\u001B[0m \u001B[43m        \u001B[49m\u001B[43mengine_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mengine_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    500\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    501\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m engine \u001B[38;5;129;01mand\u001B[39;00m engine \u001B[38;5;241m!=\u001B[39m io\u001B[38;5;241m.\u001B[39mengine:\n\u001B[1;32m    502\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    503\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEngine should not be specified when passing \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    504\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124man ExcelFile - ExcelFile already has the engine set\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    505\u001B[0m     )\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/pandas/io/excel/_base.py:1550\u001B[0m, in \u001B[0;36mExcelFile.__init__\u001B[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001B[0m\n\u001B[1;32m   1548\u001B[0m     ext \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxls\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1549\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1550\u001B[0m     ext \u001B[38;5;241m=\u001B[39m \u001B[43minspect_excel_format\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1551\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcontent_or_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\n\u001B[1;32m   1552\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1553\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ext \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1554\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1555\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExcel file format cannot be determined, you must specify \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1556\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124man engine manually.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1557\u001B[0m         )\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/pandas/io/excel/_base.py:1402\u001B[0m, in \u001B[0;36minspect_excel_format\u001B[0;34m(content_or_path, storage_options)\u001B[0m\n\u001B[1;32m   1399\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(content_or_path, \u001B[38;5;28mbytes\u001B[39m):\n\u001B[1;32m   1400\u001B[0m     content_or_path \u001B[38;5;241m=\u001B[39m BytesIO(content_or_path)\n\u001B[0;32m-> 1402\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1403\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcontent_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[1;32m   1404\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m handle:\n\u001B[1;32m   1405\u001B[0m     stream \u001B[38;5;241m=\u001B[39m handle\u001B[38;5;241m.\u001B[39mhandle\n\u001B[1;32m   1406\u001B[0m     stream\u001B[38;5;241m.\u001B[39mseek(\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/pandas/io/common.py:882\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    873\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(\n\u001B[1;32m    874\u001B[0m             handle,\n\u001B[1;32m    875\u001B[0m             ioargs\u001B[38;5;241m.\u001B[39mmode,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    878\u001B[0m             newline\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    879\u001B[0m         )\n\u001B[1;32m    880\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    881\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m--> 882\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    883\u001B[0m     handles\u001B[38;5;241m.\u001B[39mappend(handle)\n\u001B[1;32m    885\u001B[0m \u001B[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001B[39;00m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\HP\\\\OneDrive\\\\Desktop\\\\VERO_code\\\\Phase_1\\\\outputs\\\\recommendations\\\\variable_action_plan.xlsx'"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from pandas.api.types import is_numeric_dtype, is_categorical_dtype\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Paths (adjust if needed) ---\n",
    "DATA_PATH = Path(r\"C:\\Users\\HP\\OneDrive\\Desktop\\VERO_code\\Phase_1\\data\\processed\\cleaned_data.xlsx\")\n",
    "PLAN_PATH = Path(r\"C:\\Users\\HP\\OneDrive\\Desktop\\VERO_code\\Phase_1\\outputs\\recommendations\\variable_action_plan.xlsx\")  # uploaded here\n",
    "OUT_DIR   = Path(r\"C:\\Users\\HP\\OneDrive\\Desktop\\VERO_code\\Phase_1\\data\\processed\\outputs\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Load data & action plan ---\n",
    "plan_all = pd.read_excel(PLAN_PATH, sheet_name=None)\n",
    "if \"action_plan\" in plan_all:\n",
    "    action_plan_df = plan_all[\"action_plan\"].copy()\n",
    "else:\n",
    "    # fallback to the first sheet if 'action_plan' is not present\n",
    "    first_sheet = list(plan_all.keys())[0]\n",
    "    action_plan_df = plan_all[first_sheet].copy()\n",
    "\n",
    "df = pd.read_excel(DATA_PATH)\n",
    "\n",
    "print(f\"Loaded data shape: {df.shape}\")\n",
    "print(f\"Action plan rows: {len(action_plan_df)}\")\n",
    "\n",
    "# --- Normalization helper ---\n",
    "def norm(s):\n",
    "    return str(s).strip().lower().replace(\"\\u00a0\",\" \").replace(\"\\u200b\",\"\")\n",
    "\n",
    "# Normalize plan headers\n",
    "action_plan_df.columns = [norm(c) for c in action_plan_df.columns]\n",
    "if \"variable\" not in action_plan_df.columns or \"recommended_action\" not in action_plan_df.columns:\n",
    "    raise KeyError(\"Action plan must contain 'variable' and 'recommended_action' columns (case-insensitive).\")\n",
    "\n",
    "# Build lookup from normalized variable -> original recommended action\n",
    "action_plan_df[\"variable_norm\"] = action_plan_df[\"variable\"].map(norm)\n",
    "plan_lookup = (\n",
    "    action_plan_df\n",
    "      .dropna(subset=[\"variable_norm\",\"recommended_action\"])\n",
    "      .drop_duplicates(subset=[\"variable_norm\"], keep=\"first\")\n",
    "      .set_index(\"variable_norm\")[\"recommended_action\"]\n",
    "      .to_dict()\n",
    ")\n",
    "\n",
    "# Columns missing from plan\n",
    "missing_in_plan = [c for c in df.columns if norm(c) not in plan_lookup]\n",
    "print(f\"Columns missing in plan: {len(missing_in_plan)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7605f2",
   "metadata": {},
   "source": [
    "## Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc55ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic profiling summary per column\n",
    "profile_rows = []\n",
    "for c in df.columns:\n",
    "    s = df[c]\n",
    "    entry = {\n",
    "        \"column\": c,\n",
    "        \"dtype\": str(s.dtype),\n",
    "        \"n_missing\": int(s.isna().sum()),\n",
    "        \"pct_missing\": float(s.isna().mean()) * 100,\n",
    "        \"n_unique\": int(s.nunique(dropna=True))\n",
    "    }\n",
    "    if is_numeric_dtype(s):\n",
    "        entry.update({\n",
    "            \"min\": s.min(skipna=True),\n",
    "            \"q1\": s.quantile(0.25),\n",
    "            \"median\": s.median(),\n",
    "            \"q3\": s.quantile(0.75),\n",
    "            \"max\": s.max(skipna=True),\n",
    "            \"mean\": s.mean(skipna=True),\n",
    "            \"std\": s.std(skipna=True),\n",
    "        })\n",
    "    profile_rows.append(entry)\n",
    "\n",
    "profile_df = pd.DataFrame(profile_rows).sort_values(\"column\")\n",
    "profile_path = OUT_DIR / \"profiling_summary.xlsx\"\n",
    "with pd.ExcelWriter(profile_path) as xw:\n",
    "    profile_df.to_excel(xw, sheet_name=\"profile\", index=False)\n",
    "print(f\"Saved profiling summary -> {profile_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d213ff9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "numeric_columns = [col for col in df.columns if is_numeric_dtype(df[col])]\n",
    "\n",
    "print(\"numeric_columns = [\")\n",
    "for col in numeric_columns:\n",
    "    print(f\"    '{col}',\")\n",
    "print(\"]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bc72a1",
   "metadata": {},
   "source": [
    "## Plausibility Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e1a828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "# numeric plausibility bounds (case-insensitive keys)\n",
    "plaus_rules = {\n",
    "    \"active_principles_n\": (0, 50),\n",
    "    \"adr_chemo_correlation\": (0, 3),\n",
    "    \"adr_ctcae_grade\": (1, 5),\n",
    "    \"adr_left\": (0, 1),\n",
    "    \"adr_n_grado1\": (0, 50),\n",
    "    \"adr_n_grado2\": (0, 50),\n",
    "    \"adr_n_grado3\": (0, 50),\n",
    "    \"adr_n_grado4\": (0, 50),\n",
    "    \"adr_n_grado5\": (0, 50),\n",
    "    \"adr_n_tot\": (0, 100),\n",
    "    \"age\": (0, 120),\n",
    "    \"bmi_value\": (10, 80),\n",
    "    \"altre_pat_n\": (0, 50),\n",
    "    \"altro\": (0, 1),\n",
    "    \"chemio_fine_tossicita\": (0, 1),\n",
    "    \"chemo_cycles_n\": (0, 50),\n",
    "    \"comorbidity_category_list\": (0, 50),\n",
    "    \"comorbilita_cat\": (0, 20),\n",
    "    \"farmaci_cat_n\": (0, 50),\n",
    "    \"gastroesophageal_reflux_full\": (0, 1),\n",
    "    \"hypertensive_heart_disease\": (0, 1),\n",
    "    \"intervento_chirurgico_altro\": (0, 1),\n",
    "    \"linea_trattamento_oncologico\": (0, 10),\n",
    "    \"observation_days\": (0, 3650),\n",
    "    \"oncology_treatment_lines_n\": (0, 10),\n",
    "    \"ordinary_hospitalizations_n\": (0, 50),\n",
    "    \"pregresso_numero_linee_trattamento\": (0, 10),\n",
    "    \"radiotherapy_status\": (0, 2),\n",
    "    \"ricovero_n\": (0, 50),\n",
    "    \"smoking_status_binary\": (0, 1),\n",
    "    \"smoking_years\": (0, 80),\n",
    "    \"tipo_left\": (0, 1),\n",
    "    \"transfusions_total_n\": (0, 100),\n",
    "    \"treatment_line_n\": (0, 10),\n",
    "}\n",
    "\n",
    "# --- Build summary ---\n",
    "rule_lookup = {k.lower(): v for k, v in plaus_rules.items()}\n",
    "\n",
    "rows = []\n",
    "for col in df.columns:\n",
    "    col_key = str(col).strip().lower()\n",
    "    if col_key not in rule_lookup:\n",
    "        continue\n",
    "    s = df[col]\n",
    "    if not is_numeric_dtype(s):\n",
    "        continue\n",
    "\n",
    "    lo, hi = rule_lookup[col_key]\n",
    "    n_total = len(s)\n",
    "    n_nonmissing = int(s.notna().sum())\n",
    "    n_missing = n_total - n_nonmissing\n",
    "\n",
    "    if n_nonmissing > 0:\n",
    "        within = (s >= lo) & (s <= hi)\n",
    "        within = within[s.notna()]  # drop NaNs\n",
    "        n_within = int(within.sum())\n",
    "        n_outside = int(n_nonmissing - n_within)\n",
    "        pct_within_nonmissing = (n_within / n_nonmissing) * 100\n",
    "        pct_outside_nonmissing = (n_outside / n_nonmissing) * 100\n",
    "        pct_within_allrows = (n_within / n_total) * 100\n",
    "        pct_outside_allrows = (n_outside / n_total) * 100\n",
    "        observed_min = float(s.min(skipna=True))\n",
    "        observed_max = float(s.max(skipna=True))\n",
    "    else:\n",
    "        n_within = 0\n",
    "        n_outside = 0\n",
    "        pct_within_nonmissing = np.nan\n",
    "        pct_outside_nonmissing = np.nan\n",
    "        pct_within_allrows = 0.0\n",
    "        pct_outside_allrows = 0.0\n",
    "        observed_min = np.nan\n",
    "        observed_max = np.nan\n",
    "\n",
    "    pct_missing_allrows = (n_missing / n_total) * 100\n",
    "\n",
    "    rows.append({\n",
    "        \"column\": col,\n",
    "        \"min_allowed\": float(lo),\n",
    "        \"max_allowed\": float(hi),\n",
    "        \"n_total_rows\": n_total,\n",
    "        \"n_nonmissing\": n_nonmissing,\n",
    "        \"n_missing\": n_missing,\n",
    "        \"n_within\": n_within,\n",
    "        \"n_outside\": n_outside,\n",
    "        \"pct_within_nonmissing\": round(pct_within_nonmissing, 2) if pd.notna(pct_within_nonmissing) else np.nan,\n",
    "        \"pct_outside_nonmissing\": round(pct_outside_nonmissing, 2) if pd.notna(pct_outside_nonmissing) else np.nan,\n",
    "        \"pct_within_allrows\": round(pct_within_allrows, 2),\n",
    "        \"pct_outside_allrows\": round(pct_outside_allrows, 2),\n",
    "        \"pct_missing_allrows\": round(pct_missing_allrows, 2),\n",
    "        \"observed_min\": observed_min if pd.isna(observed_min) else round(observed_min, 2),\n",
    "        \"observed_max\": observed_max if pd.isna(observed_max) else round(observed_max, 2),\n",
    "    })\n",
    "\n",
    "summary = pd.DataFrame(rows)\n",
    "\n",
    "# Optional: sort like your example (most outside first, then by column)\n",
    "summary = summary.sort_values(\n",
    "    by=[\"pct_outside_nonmissing\", \"pct_missing_allrows\", \"column\"],\n",
    "    ascending=[False, False, True]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Reorder columns exactly as your sample\n",
    "summary = summary[[\n",
    "    \"column\", \"min_allowed\", \"max_allowed\",\n",
    "    \"n_total_rows\", \"n_nonmissing\", \"n_missing\",\n",
    "    \"n_within\", \"n_outside\",\n",
    "    \"pct_within_nonmissing\", \"pct_outside_nonmissing\",\n",
    "    \"pct_within_allrows\", \"pct_outside_allrows\", \"pct_missing_allrows\",\n",
    "    \"observed_min\", \"observed_max\"\n",
    "]]\n",
    "\n",
    "# --- Save to Excel ---\n",
    "out_path = Path(r\"C:\\Users\\HP\\OneDrive\\Desktop\\VERO_code\\Phase_1\\data\\processed\\outputs\\plausibility_flags_summary.xlsx\")\n",
    "with pd.ExcelWriter(out_path) as xw:\n",
    "    summary.to_excel(xw, sheet_name=\"flag_summary\", index=False)\n",
    "\n",
    "print(f\"✅ Saved: {out_path}\")\n",
    "\n",
    "\n",
    "print(summary.head(20).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb6efd1",
   "metadata": {},
   "source": [
    "## Outlier Governance (IQR Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512a9840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from pathlib import Path\n",
    "\n",
    "# Assumptions: df already exists, and OUT_DIR exists (same as in your plausibility script)\n",
    "# If not, define:\n",
    "# OUT_DIR = Path(r\"C:\\Users\\HP\\OneDrive\\Desktop\\VERO_code\\Phase_1\\outputs\")\n",
    "# OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def iqr_outlier_flags_and_summary(\n",
    "    df_in: pd.DataFrame,\n",
    "    multiplier: float = 1.5,\n",
    "    min_nonmissing: int = 8,\n",
    "    skip_binary: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      flags_df: DataFrame of 0/1 flags per column with suffix '_iqr_outlier_flag'\n",
    "      summary_df: counts and percentages per column\n",
    "      thresholds_df: per-column Q1/Q3/IQR and computed low/high thresholds\n",
    "    Notes:\n",
    "      - Only numeric columns considered\n",
    "      - Requires at least `min_nonmissing` non-missing values to compute IQR\n",
    "      - Optionally skips binary-like columns (<=2 unique non-null values)\n",
    "    \"\"\"\n",
    "    flags = {}\n",
    "    summaries = []\n",
    "    thresholds = []\n",
    "\n",
    "    for col in df_in.columns:\n",
    "        s = df_in[col]\n",
    "        if not is_numeric_dtype(s):\n",
    "            continue\n",
    "\n",
    "        # skip binary/dummy columns if requested\n",
    "        nunique_nonnull = s.dropna().nunique()\n",
    "        if skip_binary and nunique_nonnull <= 2:\n",
    "            continue\n",
    "\n",
    "        n_total = len(s)\n",
    "        n_nonmissing = int(s.notna().sum())\n",
    "        n_missing = n_total - n_nonmissing\n",
    "\n",
    "        if n_nonmissing < min_nonmissing:\n",
    "            # Not enough data to compute stable IQR thresholds\n",
    "            continue\n",
    "\n",
    "        q1 = s.quantile(0.25)\n",
    "        q3 = s.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "\n",
    "        if pd.isna(iqr) or iqr == 0:\n",
    "            # No spread -> do not flag as outliers\n",
    "            continue\n",
    "\n",
    "        low = q1 - multiplier * iqr\n",
    "        high = q3 + multiplier * iqr\n",
    "\n",
    "        # Flag outliers only among non-missing\n",
    "        outlier_mask = (s < low) | (s > high)\n",
    "        flags[f\"{col}_iqr_outlier_flag\"] = outlier_mask.fillna(False).astype(int)\n",
    "\n",
    "        n_outliers = int(outlier_mask.sum(skipna=True))\n",
    "        n_inliers = int(n_nonmissing - n_outliers)\n",
    "\n",
    "        pct_outliers_nonmissing = (n_outliers / n_nonmissing) * 100 if n_nonmissing > 0 else np.nan\n",
    "        pct_outliers_allrows = (n_outliers / n_total) * 100 if n_total > 0 else np.nan\n",
    "        pct_missing_allrows = (n_missing / n_total) * 100 if n_total > 0 else np.nan\n",
    "\n",
    "        observed_min = s.min(skipna=True)\n",
    "        observed_max = s.max(skipna=True)\n",
    "\n",
    "        summaries.append({\n",
    "            \"column\": col,\n",
    "            \"iqr_multiplier\": multiplier,\n",
    "            \"n_total_rows\": n_total,\n",
    "            \"n_nonmissing\": n_nonmissing,\n",
    "            \"n_missing\": n_missing,\n",
    "            \"n_inliers\": n_inliers,\n",
    "            \"n_outliers\": n_outliers,\n",
    "            \"pct_outliers_nonmissing\": round(pct_outliers_nonmissing, 2) if pd.notna(pct_outliers_nonmissing) else np.nan,\n",
    "            \"pct_outliers_allrows\": round(pct_outliers_allrows, 2) if pd.notna(pct_outliers_allrows) else np.nan,\n",
    "            \"pct_missing_allrows\": round(pct_missing_allrows, 2) if pd.notna(pct_missing_allrows) else np.nan,\n",
    "            \"observed_min\": round(float(observed_min), 4) if pd.notna(observed_min) else np.nan,\n",
    "            \"observed_max\": round(float(observed_max), 4) if pd.notna(observed_max) else np.nan,\n",
    "        })\n",
    "\n",
    "        thresholds.append({\n",
    "            \"column\": col,\n",
    "            \"q1\": round(float(q1), 4) if pd.notna(q1) else np.nan,\n",
    "            \"q3\": round(float(q3), 4) if pd.notna(q3) else np.nan,\n",
    "            \"iqr\": round(float(iqr), 4) if pd.notna(iqr) else np.nan,\n",
    "            \"low_threshold\": round(float(low), 4) if pd.notna(low) else np.nan,\n",
    "            \"high_threshold\": round(float(high), 4) if pd.notna(high) else np.nan,\n",
    "        })\n",
    "\n",
    "    flags_df = pd.DataFrame(flags) if flags else pd.DataFrame(index=df_in.index)\n",
    "    summary_df = pd.DataFrame(summaries) if summaries else pd.DataFrame(columns=[\n",
    "        \"column\",\"iqr_multiplier\",\"n_total_rows\",\"n_nonmissing\",\"n_missing\",\n",
    "        \"n_inliers\",\"n_outliers\",\"pct_outliers_nonmissing\",\"pct_outliers_allrows\",\n",
    "        \"pct_missing_allrows\",\"observed_min\",\"observed_max\"\n",
    "    ])\n",
    "    thresholds_df = pd.DataFrame(thresholds) if thresholds else pd.DataFrame(columns=[\n",
    "        \"column\",\"q1\",\"q3\",\"iqr\",\"low_threshold\",\"high_threshold\"\n",
    "    ])\n",
    "\n",
    "    # Sort summary by highest outlier rate\n",
    "    if not summary_df.empty:\n",
    "        summary_df = summary_df.sort_values(\n",
    "            by=[\"pct_outliers_nonmissing\",\"pct_missing_allrows\",\"column\"],\n",
    "            ascending=[False, False, True]\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "    return flags_df, summary_df, thresholds_df\n",
    "\n",
    "\n",
    "# ---- Run and Save ----\n",
    "outlier_flags, outlier_summary, outlier_thresholds = iqr_outlier_flags_and_summary(\n",
    "    df_plaus if 'df_plaus' in globals() else df,  # use df_plaus if you’re chaining after plausibility\n",
    "    multiplier=1.5,\n",
    "    min_nonmissing=8,\n",
    "    skip_binary=True\n",
    ")\n",
    "\n",
    "out_path = OUT_DIR / \"outlier_flags_summary.xlsx\"\n",
    "with pd.ExcelWriter(out_path) as xw:\n",
    "    # Flags matrix (0/1)\n",
    "    (outlier_flags if not outlier_flags.empty else pd.DataFrame()).to_excel(\n",
    "        xw, sheet_name=\"flags\", index=False\n",
    "    )\n",
    "    # Column-level counts and percentages\n",
    "    (outlier_summary if not outlier_summary.empty else pd.DataFrame()).to_excel(\n",
    "        xw, sheet_name=\"outlier_counts\", index=False\n",
    "    )\n",
    "    # Thresholds used per column\n",
    "    (outlier_thresholds if not outlier_thresholds.empty else pd.DataFrame()).to_excel(\n",
    "        xw, sheet_name=\"thresholds\", index=False\n",
    "    )\n",
    "\n",
    "print(f\"✅ Outlier flags saved -> {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9e7784b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced bmi_value max (150.695) with median (24.08).\n",
      "✅ All numeric missing values have been imputed with column medians.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "# 1. Replace extreme value in bmi_value\n",
    "# Compute median excluding NaN\n",
    "bmi_median = df['bmi_value'].median(skipna=True)\n",
    "\n",
    "# Identify the current maximum\n",
    "bmi_max = df['bmi_value'].max(skipna=True)\n",
    "\n",
    "# Replace ONLY the max value with the median\n",
    "df.loc[df['bmi_value'] == bmi_max, 'bmi_value'] = bmi_median\n",
    "\n",
    "print(f\"Replaced bmi_value max ({bmi_max}) with median ({bmi_median}).\")\n",
    "\n",
    "# 2. Median-impute all remaining missing numeric variables\n",
    "for col in df.columns:\n",
    "    if is_numeric_dtype(df[col]):\n",
    "        median_val = df[col].median(skipna=True)\n",
    "        df[col] = df[col].fillna(median_val)\n",
    "\n",
    "print(\"✅ All numeric missing values have been imputed with column medians.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178adc4e",
   "metadata": {},
   "source": [
    "## Imputation (Action Plan Driven)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99965378",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_13000\\540200952.py:105: UserWarning: Pandas requires version '3.0.5' or newer of 'xlsxwriter' (version '3.0.3' currently installed).\n",
      "  df_imputed.to_excel(clean_path, index=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned data -> C:\\Users\\HP\\OneDrive\\Desktop\\VERO_code\\Phase_1\\data\\processed\\outputs\\cleaned_imputed.xlsx\n",
      "Saved imputation log -> C:\\Users\\HP\\OneDrive\\Desktop\\VERO_code\\Phase_1\\data\\processed\\outputs\\imputation_log.txt\n",
      "Saved missingness summary -> C:\\Users\\HP\\OneDrive\\Desktop\\VERO_code\\Phase_1\\data\\processed\\outputs\\missingness_before_after.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_13000\\540200952.py:106: UserWarning: Pandas requires version '3.0.5' or newer of 'xlsxwriter' (version '3.0.3' currently installed).\n",
      "  miss_summary.to_excel(miss_path)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pandas.api.types import is_numeric_dtype, is_categorical_dtype\n",
    "\n",
    "# ---------- Config ----------\n",
    "# Use df_plaus if defined; else use df\n",
    "BASE_DF = globals().get(\"df_plaus\", globals().get(\"df\", None))\n",
    "if BASE_DF is None:\n",
    "    raise RuntimeError(\"No dataframe found. Define either `df_plaus` or `df` before running.\")\n",
    "\n",
    "# Ensure OUT_DIR exists\n",
    "OUT_DIR = globals().get(\"OUT_DIR\", Path(r\"C:\\Users\\HP\\OneDrive\\Desktop\\VERO_code\\Phase_1\\outputs\"))\n",
    "OUT_DIR = Path(OUT_DIR)\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Optionally: if you have an action plan DataFrame, convert it to a lookup dict here.\n",
    "# Expected columns: 'variable' and 'recommended_action'\n",
    "# If you already have plan_lookup, you can skip this block.\n",
    "if \"action_plan_df\" in globals():\n",
    "    plan_lookup = {\n",
    "        str(v).strip().lower(): str(a).strip()\n",
    "        for v, a in zip(action_plan_df[\"variable\"], action_plan_df[\"recommended_action\"])\n",
    "        if pd.notna(v) and pd.notna(a)\n",
    "    }\n",
    "elif \"plan_lookup\" in globals():\n",
    "    # keep user-provided plan_lookup\n",
    "    plan_lookup = globals()[\"plan_lookup\"]\n",
    "else:\n",
    "    raise RuntimeError(\"No action plan found. Provide `action_plan_df` with columns \"\n",
    "                       \"['variable','recommended_action'] or a `plan_lookup` dict.\")\n",
    "\n",
    "def norm(x) -> str:\n",
    "    return str(x).strip().lower()\n",
    "\n",
    "def impute_with_plan(df_input: pd.DataFrame, plan_lookup: dict):\n",
    "    \"\"\"\n",
    "    Actions supported (case-insensitive):\n",
    "      - 'impute' : mode for categoricals, median for numerics\n",
    "      - 'drop' or 'remove' : drop the column\n",
    "      - 'flag' or 'indicator' : create <col>_missing_flag (1 if missing)\n",
    "    \"\"\"\n",
    "    df_out = df_input.copy()\n",
    "    logs = []\n",
    "\n",
    "    # Snapshot of columns to avoid mutation-during-iteration issues\n",
    "    for raw_col in list(df_out.columns):\n",
    "        col_norm = norm(raw_col)\n",
    "        miss_n = int(df_out[raw_col].isna().sum())\n",
    "        if miss_n == 0:\n",
    "            continue\n",
    "\n",
    "        action = plan_lookup.get(col_norm)\n",
    "        if not action:\n",
    "            logs.append(f\"[SKIP] {raw_col}: no action configured in plan\")\n",
    "            continue\n",
    "\n",
    "        action_norm = norm(action)\n",
    "\n",
    "        if action_norm == \"impute\":\n",
    "            s = df_out[raw_col]\n",
    "            if s.dtype == \"object\" or is_categorical_dtype(s):\n",
    "                mode_val = s.mode(dropna=True)\n",
    "                if not mode_val.empty:\n",
    "                    df_out[raw_col] = s.fillna(mode_val.iloc[0])\n",
    "                    logs.append(f\"[IMPUTE-MODE] {raw_col} -> {mode_val.iloc[0]} (filled {miss_n})\")\n",
    "                else:\n",
    "                    logs.append(f\"[WARN] {raw_col}: mode empty, no fill performed\")\n",
    "            elif is_numeric_dtype(s):\n",
    "                med = s.median(skipna=True)\n",
    "                df_out[raw_col] = s.fillna(med)\n",
    "                logs.append(f\"[IMPUTE-MEDIAN] {raw_col} -> {med} (filled {miss_n})\")\n",
    "            else:\n",
    "                logs.append(f\"[SKIP] {raw_col}: unsupported dtype for impute\")\n",
    "\n",
    "        elif action_norm in {\"drop\", \"remove\"}:\n",
    "            df_out = df_out.drop(columns=[raw_col])\n",
    "            logs.append(f\"[DROP] {raw_col}\")\n",
    "\n",
    "        elif action_norm in {\"flag\", \"indicator\"}:\n",
    "            flag_col = f\"{raw_col}_missing_flag\"\n",
    "            df_out[flag_col] = df_out[raw_col].isna().astype(int)\n",
    "            logs.append(f\"[FLAG] {raw_col} -> {flag_col} (created indicator)\")\n",
    "\n",
    "        else:\n",
    "            logs.append(f\"[SKIP] {raw_col}: unrecognized action '{action}'\")\n",
    "\n",
    "    return df_out, logs\n",
    "\n",
    "# ---------- Run ----------\n",
    "before_missing = BASE_DF.isna().sum()\n",
    "df_imputed, impute_logs = impute_with_plan(BASE_DF, plan_lookup)\n",
    "after_missing = df_imputed.isna().sum()\n",
    "\n",
    "miss_summary = (\n",
    "    pd.DataFrame({\"missing_before\": before_missing, \"missing_after\": after_missing})\n",
    "      .assign(delta=lambda d: d[\"missing_after\"] - d[\"missing_before\"])\n",
    "      .sort_values(\"delta\")\n",
    ")\n",
    "\n",
    "# ---------- Save ----------\n",
    "clean_path = OUT_DIR / \"cleaned_imputed.xlsx\"\n",
    "log_path = OUT_DIR / \"imputation_log.txt\"\n",
    "miss_path = OUT_DIR / \"missingness_before_after.xlsx\"\n",
    "\n",
    "df_imputed.to_excel(clean_path, index=False)\n",
    "miss_summary.to_excel(miss_path)\n",
    "\n",
    "with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in impute_logs:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "print(f\"Saved cleaned data -> {clean_path}\")\n",
    "print(f\"Saved imputation log -> {log_path}\")\n",
    "print(f\"Saved missingness summary -> {miss_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4eb8f10",
   "metadata": {},
   "source": [
    "## Combined Report Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94996600",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_13000\\885773855.py:2: UserWarning: Pandas requires version '3.0.5' or newer of 'xlsxwriter' (version '3.0.3' currently installed).\n",
      "  with pd.ExcelWriter(report_path) as xw:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved combined report -> C:\\Users\\HP\\OneDrive\\Desktop\\VERO_code\\Phase_1\\data\\processed\\outputs\\data_quality_report.xlsx\n"
     ]
    }
   ],
   "source": [
    "report_path = OUT_DIR / \"data_quality_report.xlsx\"\n",
    "with pd.ExcelWriter(report_path) as xw:\n",
    "\n",
    "    # Always include profiling\n",
    "    profile_df.to_excel(xw, sheet_name=\"profile\", index=False)\n",
    "\n",
    "    # ---- Plausibility Flags + Counts ----\n",
    "    if 'plaus_flags' in globals() and isinstance(plaus_flags, pd.DataFrame) and not plaus_flags.empty:\n",
    "        plaus_flags.to_excel(xw, sheet_name=\"plausibility_flags\", index=False)\n",
    "\n",
    "        # Ensure plaus_summary is DataFrame\n",
    "        if isinstance(plaus_summary, pd.Series):\n",
    "            plaus_summary.to_frame(\"n_flags\").to_excel(xw, sheet_name=\"plausibility_counts\")\n",
    "        else:\n",
    "            # already DataFrame\n",
    "            plaus_summary.to_excel(xw, sheet_name=\"plausibility_counts\", index=False)\n",
    "\n",
    "    # ---- Outlier Flags + Counts ----\n",
    "    if 'outlier_flags' in globals() and isinstance(outlier_flags, pd.DataFrame) and not outlier_flags.empty:\n",
    "        outlier_flags.to_excel(xw, sheet_name=\"outlier_flags\", index=False)\n",
    "\n",
    "        if isinstance(outlier_summary, pd.Series):\n",
    "            outlier_summary.to_frame(\"n_outliers\").to_excel(xw, sheet_name=\"outlier_counts\")\n",
    "        else:\n",
    "            outlier_summary.to_excel(xw, sheet_name=\"outlier_counts\", index=False)\n",
    "\n",
    "    # ---- Missingness Summary ----\n",
    "    miss_summary.to_excel(xw, sheet_name=\"missingness_before_after\", index=True)\n",
    "\n",
    "print(f\"✅ Saved combined report -> {report_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
