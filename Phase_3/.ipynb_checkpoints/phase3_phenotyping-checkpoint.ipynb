{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec5a666a",
   "metadata": {},
   "source": [
    "\n",
    "# Phase 3 - Aging Phenotypes: Gower Distance, Dimensionality Reduction, Clustering, and Validation\n",
    "\n",
    "This notebook takes the integrated Phase 3 dataset and:\n",
    "1. Computes a mixed-type **Gower distance** matrix\n",
    "2. Runs **PCA** on numeric features for interpretability\n",
    "3. Runs **UMAP** on the Gower distance for structure visualization (falls back to t-SNE if UMAP is unavailable)\n",
    "4. Performs **Hierarchical Clustering** (average linkage) on the Gower distance and selects k (2..8) by silhouette\n",
    "5. Assesses **stability** with bootstrap resampling (Hungarian matching + Jaccard)\n",
    "6. Generates **validation summaries** and exports artifacts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416f10c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Try to import UMAP if available\n",
    "try:\n",
    "    import umap\n",
    "    HAS_UMAP = True\n",
    "except Exception:\n",
    "    HAS_UMAP = False\n",
    "\n",
    "BASE = Path(\"/mnt/data\")\n",
    "ART = BASE / \"phase3_artifacts\"\n",
    "ART.mkdir(exist_ok=True)\n",
    "\n",
    "INTEGRATED_PATH = ART / \"phase3_integrated_data.csv\"\n",
    "TYPES_PATH = BASE / \"features_data_types.xlsx\"\n",
    "\n",
    "# Clustering/k selection\n",
    "K_MIN, K_MAX = 2, 8\n",
    "BOOTSTRAP_N = 100  # stability iterations (can increase later)\n",
    "RANDOM_STATE = 7\n",
    "\n",
    "print(\"Using UMAP:\", HAS_UMAP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f57984e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(INTEGRATED_PATH)\n",
    "types = pd.read_excel(TYPES_PATH)\n",
    "\n",
    "# Identify id column\n",
    "id_candidates = [c for c in df.columns if re.search(r\"(patient.*id|id.*patient|^id$)\", c, flags=re.I)]\n",
    "id_col = id_candidates[0] if id_candidates else None\n",
    "print(\"ID column:\", id_col)\n",
    "\n",
    "# Use features_data_types.xlsx to define numeric vs categorical\n",
    "types = types.rename(columns={\n",
    "    \"Feature\": \"feature_orig\",\n",
    "    \"Data Type\": \"Data Type\"\n",
    "}) if \"Feature\" in types.columns else types\n",
    "\n",
    "types[\"feature_norm\"] = types.get(\"feature_orig\", types.iloc[:,0]).astype(str).str.strip().str.lower()\n",
    "df_cols_norm = pd.Series(df.columns, index=df.columns).astype(str).str.strip().str.lower()\n",
    "types = types[types[\"feature_norm\"].isin(df_cols_norm.values)]\n",
    "\n",
    "# Map dtypes\n",
    "dtype_map = {}\n",
    "for _, row in types.iterrows():\n",
    "    f = row[\"feature_norm\"]\n",
    "    # try to find the exact column in df that matches this normalized name\n",
    "    match = [c for c in df.columns if c.strip().lower() == f]\n",
    "    if match:\n",
    "        col = match[0]\n",
    "        dt = str(row.get(\"Data Type\", \"\"))\n",
    "        dtype_map[col] = \"numeric\" if re.search(r\"(int|float)\", dt, flags=re.I) else \"categorical\"\n",
    "\n",
    "# If types missed some columns, infer by pandas dtype\n",
    "for c in df.columns:\n",
    "    if c == id_col:\n",
    "        continue\n",
    "    if c not in dtype_map:\n",
    "        dtype_map[c] = \"numeric\" if pd.api.types.is_numeric_dtype(df[c]) else \"categorical\"\n",
    "\n",
    "# Split\n",
    "feature_cols = [c for c in df.columns if c != id_col]\n",
    "num_cols = [c for c in feature_cols if dtype_map.get(c, \"numeric\") == \"numeric\"]\n",
    "cat_cols = [c for c in feature_cols if dtype_map.get(c, \"numeric\") != \"numeric\"]\n",
    "\n",
    "len(feature_cols), len(num_cols), len(cat_cols), feature_cols[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae50483e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def minmax_scale(arr):\n",
    "    a = arr.astype(float)\n",
    "    mn = np.nanmin(a, axis=0)\n",
    "    mx = np.nanmax(a, axis=0)\n",
    "    denom = (mx - mn)\n",
    "    denom[denom == 0] = 1.0\n",
    "    return (a - mn) / denom\n",
    "\n",
    "def gower_distance(X_num, X_cat):\n",
    "    n = X_num.shape[0] if X_num is not None else X_cat.shape[0]\n",
    "    D = np.zeros((n, n), dtype=float)\n",
    "\n",
    "    # Numeric contribution (normalized to [0,1])\n",
    "    if X_num is not None and X_num.shape[1] > 0:\n",
    "        Xn = X_num.copy().astype(float)\n",
    "        # Impute missing with column medians for distance computation\n",
    "        col_med = np.nanmedian(Xn, axis=0)\n",
    "        inds = np.where(np.isnan(Xn))\n",
    "        Xn[inds] = np.take(col_med, inds[1])\n",
    "        Xn = minmax_scale(Xn)\n",
    "\n",
    "        # Pairwise L1 distances averaged over numeric dims\n",
    "        for i in range(n):\n",
    "            diff = np.abs(Xn[i] - Xn)  # (n, p_num)\n",
    "            D += diff.mean(axis=1)\n",
    "\n",
    "    # Categorical contribution (mismatch 0/1 averaged)\n",
    "    if X_cat is not None and X_cat.shape[1] > 0:\n",
    "        Xc = X_cat.copy()\n",
    "        # Fill NaN as a separate category label\n",
    "        Xc = np.where(pd.isna(Xc), \"__NA__\", Xc).astype(object)\n",
    "        for j in range(Xc.shape[1]):\n",
    "            col = Xc[:, j]\n",
    "            # mismatch matrix: 1 if diff, 0 if same\n",
    "            mismatch = (col.reshape(-1,1) != col.reshape(1,-1)).astype(float)\n",
    "            D += mismatch\n",
    "\n",
    "        D /= ( (X_num.shape[1] if X_num is not None else 0) + Xc.shape[1] )\n",
    "\n",
    "    else:\n",
    "        # Only numeric contributed so far -> average already done above\n",
    "        if X_num is not None and X_num.shape[1] > 0:\n",
    "            D /= X_num.shape[1]\n",
    "\n",
    "    # Ensure diagonal zero and symmetry\n",
    "    np.fill_diagonal(D, 0.0)\n",
    "    return D\n",
    "\n",
    "X_num = df[num_cols].to_numpy() if len(num_cols) > 0 else None\n",
    "X_cat = df[cat_cols].astype(str).to_numpy() if len(cat_cols) > 0 else None\n",
    "\n",
    "D = gower_distance(X_num, X_cat)\n",
    "np.save(ART / \"gower_distance.npy\", D)\n",
    "print(\"Gower distance shape:\", D.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eff5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PCA for numeric interpretability\n",
    "pca_scores = None\n",
    "pca = None\n",
    "if len(num_cols) > 0:\n",
    "    scaler = StandardScaler()\n",
    "    Z = scaler.fit_transform(df[num_cols])\n",
    "    pca = PCA(n_components=min(len(num_cols), 10), random_state=RANDOM_STATE)\n",
    "    pca_scores = pca.fit_transform(Z)\n",
    "    pd.DataFrame(pca.components_, columns=num_cols).to_csv(ART / \"pca_loadings.csv\", index=False)\n",
    "    print(\"PCA done. Explained variance (first 5):\", pca.explained_variance_ratio_[:5])\n",
    "else:\n",
    "    print(\"No numeric columns for PCA.\")\n",
    "\n",
    "# UMAP embedding from precomputed Gower, fallback to t-SNE\n",
    "if HAS_UMAP:\n",
    "    reducer = umap.UMAP(metric=\"precomputed\", random_state=RANDOM_STATE, n_neighbors=30, min_dist=0.1)\n",
    "    emb = reducer.fit_transform(D)\n",
    "    emb_df = pd.DataFrame(emb, columns=[\"UMAP1\",\"UMAP2\"])\n",
    "    emb_df.to_csv(ART / \"umap_embeddings.csv\", index=False)\n",
    "    EMB_NAME = \"UMAP\"\n",
    "else:\n",
    "    tsne = TSNE(metric=\"precomputed\", random_state=RANDOM_STATE, perplexity=max(5, min(30, D.shape[0]//5)))\n",
    "    emb = tsne.fit_transform(D)\n",
    "    emb_df = pd.DataFrame(emb, columns=[\"TSNE1\",\"TSNE2\"])\n",
    "    emb_df.to_csv(ART / \"tsne_embeddings.csv\", index=False)\n",
    "    EMB_NAME = \"tSNE\"\n",
    "\n",
    "print(\"Embedding computed:\", EMB_NAME, emb_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73645868",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the 2D embedding\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(emb_df.iloc[:,0], emb_df.iloc[:,1], s=15)\n",
    "plt.xlabel(emb_df.columns[0]); plt.ylabel(emb_df.columns[1])\n",
    "plt.title(f\"{EMB_NAME} embedding of patients (Gower space)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(ART / \"embedding_plot.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# PCA scree (if PCA ran)\n",
    "if pca is not None:\n",
    "    evr = pca.explained_variance_ratio_\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(np.arange(1, len(evr)+1), np.cumsum(evr), marker=\"o\")\n",
    "    plt.xlabel(\"Number of components\")\n",
    "    plt.ylabel(\"Cumulative explained variance\")\n",
    "    plt.title(\"PCA Scree (cumulative)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ART / \"pca_scree.png\", dpi=150)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff71dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hierarchical clustering on the Gower distance\n",
    "# Convert distance to condensed form for scipy.linkage requires a condensed distance vector\n",
    "def square_to_condensed(square):\n",
    "    # Upper triangle without diagonal\n",
    "    idx = np.triu_indices_from(square, k=1)\n",
    "    return square[idx]\n",
    "\n",
    "condensed = square_to_condensed(D)\n",
    "Z = linkage(condensed, method=\"average\")\n",
    "\n",
    "# Try k from K_MIN..K_MAX\n",
    "results = []\n",
    "for k in range(K_MIN, K_MAX+1):\n",
    "    labels = fcluster(Z, k, criterion=\"maxclust\")\n",
    "    sil = silhouette_score(D, labels, metric=\"precomputed\")\n",
    "    results.append((k, sil))\n",
    "sel_df = pd.DataFrame(results, columns=[\"k\",\"silhouette\"]).sort_values(\"silhouette\", ascending=False)\n",
    "sel_df.to_csv(ART / \"k_silhouette_scan.csv\", index=False)\n",
    "sel_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530b4b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_k = int(sel_df.iloc[0][\"k\"])\n",
    "labels_best = fcluster(Z, best_k, criterion=\"maxclust\")\n",
    "pd.DataFrame({\"cluster\": labels_best}).to_csv(ART / \"cluster_labels.csv\", index=False)\n",
    "\n",
    "print(\"Best k:\", best_k, \"Silhouette:\", float(sel_df.iloc[0][\"silhouette\"]))\n",
    "print(\"Cluster sizes:\", pd.Series(labels_best).value_counts().to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec8bc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rng = np.random.default_rng(RANDOM_STATE)\n",
    "\n",
    "def jaccard_index(a, b):\n",
    "    inter = len(set(a) & set(b))\n",
    "    union = len(set(a) | set(b))\n",
    "    return inter / union if union > 0 else 0.0\n",
    "\n",
    "def stability_bootstrap(D, labels_ref, B=BOOTSTRAP_N):\n",
    "    n = D.shape[0]\n",
    "    k = len(np.unique(labels_ref))\n",
    "    ref = labels_ref.copy()\n",
    "\n",
    "    # Build an indicator matrix per cluster for reference\n",
    "    from collections import defaultdict\n",
    "    ref_sets = [set(np.where(ref == c)[0]) for c in np.unique(ref)]\n",
    "\n",
    "    jaccards = []\n",
    "\n",
    "    for _ in range(B):\n",
    "        # sample indices with replacement\n",
    "        idx = rng.integers(0, n, size=n)\n",
    "        D_b = D[np.ix_(idx, idx)]\n",
    "        condensed_b = square_to_condensed(D_b)\n",
    "        Z_b = linkage(condensed_b, method=\"average\")\n",
    "        lab_b = fcluster(Z_b, k, criterion=\"maxclust\")\n",
    "\n",
    "        # Map bootstrap clusters back to original indices\n",
    "        boot_sets = [set(np.where(lab_b == c)[0]) for c in np.unique(lab_b)]\n",
    "\n",
    "        # Build cost matrix = 1 - Jaccard; solve assignment to maximize overlap\n",
    "        cost = np.ones((k, k))\n",
    "        for i in range(k):\n",
    "            for j in range(k):\n",
    "                # Compare in the original index space via the sampled indices\n",
    "                a = set(idx[list(boot_sets[j])])\n",
    "                b = ref_sets[i]\n",
    "                cost[i, j] = 1.0 - jaccard_index(a, b)\n",
    "\n",
    "        # Hungarian assignment\n",
    "        ri, cj = linear_sum_assignment(cost)\n",
    "        matched = [(i, j, 1.0 - cost[i, j]) for i, j in zip(ri, cj)]\n",
    "        # Average matched Jaccard\n",
    "        jaccards.append(np.mean([m[2] for m in matched]))\n",
    "\n",
    "    return float(np.mean(jaccards)), float(np.std(jaccards))\n",
    "\n",
    "stab_mean, stab_std = stability_bootstrap(D, labels_best, B=BOOTSTRAP_N)\n",
    "with open(ART / \"stability_report.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"Bootstrap stability (mean Jaccard across matched clusters): {stab_mean:.3f} Â± {stab_std:.3f}\\n\")\n",
    "print(\"Stability (mean Jaccard):\", stab_mean, \"+/-\", stab_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ec6ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Try to detect likely outcome columns for quick validation\n",
    "df_lab = df.copy()\n",
    "df_lab[\"cluster\"] = labels_best\n",
    "\n",
    "def find_cols(patterns):\n",
    "    out = []\n",
    "    for c in df.columns:\n",
    "        cl = c.lower()\n",
    "        if any(p in cl for p in patterns):\n",
    "            out.append(c)\n",
    "    return list(dict.fromkeys(out))\n",
    "\n",
    "cand_binary = find_cols([\"readmission\", \"adr\", \"toxicity\", \"frailty\", \"mortality\", \"death\", \"event\"])\n",
    "cand_cont = find_cols([\"survival_time\",\"time_to_event\",\"frailty_score\",\"risk_index\",\"c_index\"])\n",
    "\n",
    "print(\"Detected potential outcome columns:\", cand_binary + cand_cont)\n",
    "\n",
    "# Summaries\n",
    "summary_frames = []\n",
    "\n",
    "# Binary rate by cluster\n",
    "for c in cand_binary:\n",
    "    if df_lab[c].dropna().isin([0,1]).all():\n",
    "        tab = df_lab.groupby(\"cluster\")[c].mean().rename(f\"rate_{c}\")\n",
    "        summary_frames.append(tab)\n",
    "\n",
    "# Continuous mean by cluster\n",
    "for c in cand_cont:\n",
    "    if pd.api.types.is_numeric_dtype(df_lab[c]):\n",
    "        tab = df_lab.groupby(\"cluster\")[c].mean().rename(f\"mean_{c}\")\n",
    "        summary_frames.append(tab)\n",
    "\n",
    "if summary_frames:\n",
    "    summary = pd.concat(summary_frames, axis=1)\n",
    "    summary.to_csv(ART / \"cluster_outcome_summary.csv\")\n",
    "    display(summary.head())\n",
    "else:\n",
    "    print(\"No obvious outcome columns detected for quick validation. Use your known outcome fields to extend validation.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
