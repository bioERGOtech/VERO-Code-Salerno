{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28c22e08",
   "metadata": {},
   "source": [
    "\n",
    "# Phase 3 - Clinical Validation and Phenotype Reporting\n",
    "\n",
    "This notebook validates that the discovered phenotypes are clinically meaningful.\n",
    "It expects these files in `phase3_artifacts/`:\n",
    "- phase3_integrated_data.csv\n",
    "- cluster_labels.csv\n",
    "\n",
    "It will:\n",
    "1. Merge clusters into the integrated data.\n",
    "2. Detect or accept survival time and event columns.\n",
    "3. Plot Kaplan Meier curves by cluster and run pairwise log rank tests.\n",
    "4. Compare binary outcomes (ADR, readmission, toxicity, mortality) across clusters.\n",
    "5. Compare continuous outcomes (frailty score, risk index) across clusters.\n",
    "6. Summarize each phenotype into a compact card for reporting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b03fedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'phase3_artifacts\\\\phase3_integrated_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10420\\2786328823.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mCLUSTERS\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mART\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m\"cluster_labels.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mINTEGRATED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCLUSTERS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1026\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 620\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    621\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1618\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1620\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1621\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1622\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"b\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1880\u001b[1;33m             self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 873\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'phase3_artifacts\\\\phase3_integrated_data.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Optional survival analysis. Install if missing.\n",
    "try:\n",
    "    from lifelines import KaplanMeierFitter\n",
    "    from lifelines.statistics import pairwise_logrank_test\n",
    "    HAS_LIFELINES = True\n",
    "except Exception:\n",
    "    HAS_LIFELINES = False\n",
    "    print(\"lifelines not found. Install with: pip install lifelines\")\n",
    "\n",
    "BASE = Path(\".\")\n",
    "ART = BASE / \"phase3_artifacts\"\n",
    "\n",
    "INTEGRATED = ART / \"phase3_integrated_data.csv\"\n",
    "CLUSTERS   = ART / \"cluster_labels.csv\"\n",
    "\n",
    "df = pd.read_csv(INTEGRATED)\n",
    "labels = pd.read_csv(CLUSTERS)\n",
    "df = df.copy()\n",
    "df[\"cluster\"] = labels[\"cluster\"].values\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Cluster counts:\", df[\"cluster\"].value_counts().sort_index().to_dict())\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e3ed8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose survival columns. Override here if the auto-detector is wrong.\n",
    "SURV_TIME_COL = None      # e.g., \"time_to_event_days\"\n",
    "SURV_EVENT_COL = None     # e.g., \"event\" (1=event/death, 0=censored)\n",
    "\n",
    "def find_cols(patterns):\n",
    "    out = []\n",
    "    for c in df.columns:\n",
    "        cl = c.lower()\n",
    "        if any(p in cl for p in patterns):\n",
    "            out.append(c)\n",
    "    return list(dict.fromkeys(out))\n",
    "\n",
    "cand_time  = find_cols([\"survival_time\",\"time_to_event\",\"followup\",\"follow_up\",\"days_to\",\"time\"])\n",
    "cand_event = find_cols([\"event\",\"death\",\"mortality\",\"status\",\"censor\",\"censored\"])\n",
    "\n",
    "# Heuristic picks if no override is given\n",
    "if SURV_TIME_COL is None:\n",
    "    SURV_TIME_COL = next((c for c in cand_time if pd.api.types.is_numeric_dtype(df[c])), None)\n",
    "\n",
    "if SURV_EVENT_COL is None:\n",
    "    SURV_EVENT_COL = next((c for c in cand_event if set(pd.Series(df[c]).dropna().unique()).issubset({0,1})), None)\n",
    "\n",
    "print(\"Detected time column:\", SURV_TIME_COL)\n",
    "print(\"Detected event column:\", SURV_EVENT_COL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ab1459d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SURV_TIME_COL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10420\\4286665972.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Kaplan Meier curves and pairwise log-rank tests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mHAS_LIFELINES\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mSURV_TIME_COL\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mSURV_EVENT_COL\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mkmf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKaplanMeierFitter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"cluster\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SURV_TIME_COL' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Kaplan Meier curves and pairwise log-rank tests\n",
    "if HAS_LIFELINES and SURV_TIME_COL and SURV_EVENT_COL:\n",
    "    kmf = KaplanMeierFitter()\n",
    "    plt.figure(figsize=(7,5))\n",
    "    for cl in sorted(df[\"cluster\"].unique()):\n",
    "        sub = df[df[\"cluster\"] == cl]\n",
    "        kmf.fit(sub[SURV_TIME_COL], event_observed=sub[SURV_EVENT_COL], label=f\"Cluster {cl}\")\n",
    "        kmf.plot(ci_show=False)\n",
    "    plt.title(\"Kaplan Meier survival by cluster\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Survival probability\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ART / \"km_curves_by_cluster.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    # Pairwise log rank tests\n",
    "    res = pairwise_logrank_test(df[SURV_TIME_COL], df[\"cluster\"], df[SURV_EVENT_COL])\n",
    "    try:\n",
    "        res.summary.to_csv(ART / \"logrank_pairwise.csv\")\n",
    "        display(res.summary.head())\n",
    "    except Exception:\n",
    "        print(\"Wrote pairwise log-rank p values.\")\n",
    "else:\n",
    "    print(\"KM was skipped. lifelines not installed or survival columns not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da51323f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Binary outcomes. Save per cluster rates and chi square tests.\n",
    "def find_cols(patterns):\n",
    "    out = []\n",
    "    for c in df.columns:\n",
    "        cl = c.lower()\n",
    "        if any(p in cl for p in patterns):\n",
    "            out.append(c)\n",
    "    return list(dict.fromkeys(out))\n",
    "\n",
    "cand_bin = find_cols([\"readmission\",\"adr\",\"toxicity\",\"frailty\",\"mortality\",\"death\",\"event\"])\n",
    "\n",
    "tests = []\n",
    "for c in cand_bin:\n",
    "    vals = pd.Series(df[c])\n",
    "    ok = set(vals.dropna().unique()).issubset({0,1})\n",
    "    if not ok:\n",
    "        continue\n",
    "    # rate table\n",
    "    tab = df.groupby(\"cluster\")[c].agg([\"mean\",\"sum\",\"count\"]).rename(columns={\"mean\":\"rate\"})\n",
    "    outp = ART / f\"rate_by_cluster__{c}.csv\"\n",
    "    tab.to_csv(outp)\n",
    "    # chi square test\n",
    "    cont = pd.crosstab(df[\"cluster\"], df[c])\n",
    "    if cont.shape[1] == 2:\n",
    "        chi2, p, dof, _ = stats.chi2_contingency(cont)\n",
    "        tests.append({\"outcome\": c, \"chi2\": chi2, \"p_value\": p, \"dof\": dof})\n",
    "\n",
    "if tests:\n",
    "    pd.DataFrame(tests).to_csv(ART / \"binary_outcome_tests.csv\", index=False)\n",
    "    pd.DataFrame(tests).head()\n",
    "else:\n",
    "    print(\"No binary outcomes detected or they were not strictly 0 or 1.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8acf1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Continuous outcomes. Save per cluster summaries and ANOVA or Kruskal.\n",
    "cand_cont = []\n",
    "for c in df.columns:\n",
    "    if c == \"cluster\":\n",
    "        continue\n",
    "    if pd.api.types.is_numeric_dtype(df[c]):\n",
    "        cl = c.lower()\n",
    "        if any(p in cl for p in [\"frailty_score\",\"risk_index\",\"score\",\"index\",\"survival_time\",\"time_to_event\"]):\n",
    "            cand_cont.append(c)\n",
    "\n",
    "tests_c = []\n",
    "for c in cand_cont:\n",
    "    tab = df.groupby(\"cluster\")[c].agg([\"mean\",\"std\",\"median\",\"count\"])\n",
    "    tab.to_csv(ART / f\"cont_by_cluster__{c}.csv\")\n",
    "    groups = [df[df[\"cluster\"]==cl][c].dropna().values for cl in sorted(df[\"cluster\"].unique())]\n",
    "    # Try ANOVA, fallback to Kruskal\n",
    "    try:\n",
    "        f_stat, p = stats.f_oneway(*groups)\n",
    "        tests_c.append({\"outcome\": c, \"test\": \"ANOVA\", \"stat\": f_stat, \"p_value\": p})\n",
    "    except Exception:\n",
    "        H, p = stats.kruskal(*groups)\n",
    "        tests_c.append({\"outcome\": c, \"test\": \"Kruskal\", \"stat\": H, \"p_value\": p})\n",
    "\n",
    "if tests_c:\n",
    "    pd.DataFrame(tests_c).to_csv(ART / \"continuous_outcome_tests.csv\", index=False)\n",
    "    pd.DataFrame(tests_c).head()\n",
    "else:\n",
    "    print(\"No continuous outcomes matched the simple patterns. Edit `cand_cont` logic if needed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c90e612",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Phenotype cards: quick snapshot per cluster using common outcome names\n",
    "cards = []\n",
    "clusters = sorted(df[\"cluster\"].unique())\n",
    "def try_rate(col):\n",
    "    if col in df.columns and set(pd.Series(df[col]).dropna().unique()).issubset({0,1}):\n",
    "        return float(df.groupby(\"cluster\")[col].mean().to_dict().get(cl, np.nan))\n",
    "    return np.nan\n",
    "\n",
    "for cl in clusters:\n",
    "    sub = df[df[\"cluster\"]==cl]\n",
    "    card = {\n",
    "        \"cluster\": int(cl),\n",
    "        \"n_patients\": int(len(sub)),\n",
    "    }\n",
    "    for nm in [\"frailty\",\"readmission\",\"adr\",\"toxicity\",\"mortality\",\"death\",\"event\"]:\n",
    "        cols = [c for c in df.columns if nm in c.lower()]\n",
    "        for c in cols:\n",
    "            if set(pd.Series(df[c]).dropna().unique()).issubset({0,1}):\n",
    "                card[f\"rate_{c}\"] = float(sub[c].mean())\n",
    "    cards.append(card)\n",
    "\n",
    "cards_df = pd.DataFrame(cards).sort_values(\"cluster\")\n",
    "cards_df.to_csv(ART / \"phenotype_cards.csv\", index=False)\n",
    "cards_df.head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
