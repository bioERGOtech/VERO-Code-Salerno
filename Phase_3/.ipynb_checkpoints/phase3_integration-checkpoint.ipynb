{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80ed0241",
   "metadata": {},
   "source": [
    "\n",
    "# Phase 3 - Feature Integration and Selection Notebook\n",
    "\n",
    "This notebook reproduces the integration step for Phase 3:\n",
    "- Load cleaned analytic dataset and Phase 2 outputs\n",
    "- Normalize feature names\n",
    "- Merge robust signals from RI significance, univariate screening, and model coefficients\n",
    "- Score and select top features (default: 150)\n",
    "- Export the integrated dataset ready for Gower distance, UMAP, PCA, and clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5efb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Parameters\n",
    "TOP_N = 150  # number of features to keep\n",
    "BASE = Path(\"/mnt/data\")\n",
    "ART_DIR = BASE / \"phase3_artifacts\"\n",
    "ART_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Artifacts will be written to:\", ART_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a155a803",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def norm(s):\n",
    "    \"\"\"Lightweight normalization for feature keys.\"\"\"\n",
    "    if pd.isna(s):\n",
    "        return np.nan\n",
    "    s = str(s).strip().replace(\"\\n\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.lower()\n",
    "\n",
    "def rank01(series):\n",
    "    \"\"\"Convert a numeric series to percentile ranks in [0,1], tolerant to missing.\"\"\"\n",
    "    s = series.copy()\n",
    "    s = s.replace([np.inf, -np.inf], np.nan)\n",
    "    if s.notna().sum() < 3:\n",
    "        return pd.Series(np.nan, index=s.index)\n",
    "    return s.rank(pct=True)\n",
    "\n",
    "def prep_coef(df):\n",
    "    \"\"\"Prepare coefficient tables to expose a unified feature_norm and coefficient column.\"\"\"\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    feat_col = cols.get(\"feature_expanded\", None)\n",
    "    if feat_col is None:\n",
    "        candidates = [c for c in df.columns if \"feature\" in c.lower()]\n",
    "        feat_col = candidates[0] if candidates else df.columns[0]\n",
    "    df = df.copy()\n",
    "    df[\"feature_norm\"] = df[feat_col].map(norm)\n",
    "    coef_col = cols.get(\"coefficient\", None)\n",
    "    if coef_col is None:\n",
    "        alt = [c for c in df.columns if \"coef\" in c.lower()]\n",
    "        coef_col = alt[0] if alt else None\n",
    "    if coef_col:\n",
    "        df[\"coefficient\"] = pd.to_numeric(df[coef_col], errors=\"coerce\")\n",
    "    else:\n",
    "        df[\"coefficient\"] = np.nan\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3451ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Core data\n",
    "clean = pd.read_excel(BASE / \"cleaned_data.xlsx\")\n",
    "types = pd.read_excel(BASE / \"features_data_types.xlsx\")\n",
    "\n",
    "# Phase 2 outputs\n",
    "ri_sig = pd.read_excel(BASE / \"RI_Significance_Table_robust.xlsx\")\n",
    "uni_top = pd.read_excel(BASE / \"univariate_summary_top25.xlsx\")\n",
    "coef_frailty = pd.read_excel(BASE / \"coefficients_Frailty_Category.xlsx\")\n",
    "coef_adr = pd.read_excel(BASE / \"coefficients_Severe_ADRs.xlsx\")\n",
    "coef_readm = pd.read_excel(BASE / \"coefficients_readmission_flag.xlsx\")\n",
    "\n",
    "clean.shape, types.shape, ri_sig.shape, uni_top.shape, coef_frailty.shape, coef_adr.shape, coef_readm.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407b369b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build original->normalized column map\n",
    "clean_cols = pd.Series(clean.columns, name=\"feature_orig\")\n",
    "col_map = pd.DataFrame({\n",
    "    \"feature_orig\": clean_cols,\n",
    "    \"feature_norm\": clean_cols.map(norm)\n",
    "})\n",
    "\n",
    "# Prepare typing table\n",
    "if \"Feature\" in types.columns:\n",
    "    types = types.rename(columns={\"Feature\": \"feature_orig\"})\n",
    "    types[\"feature_norm\"] = types[\"feature_orig\"].map(norm)\n",
    "elif \"feature\" in types.columns:\n",
    "    types = types.rename(columns={\"feature\": \"feature_orig\"})\n",
    "    types[\"feature_norm\"] = types[\"feature_orig\"].map(norm)\n",
    "else:\n",
    "    # fallback to dtype introspection\n",
    "    types = pd.DataFrame({\n",
    "        \"feature_orig\": clean.columns,\n",
    "        \"feature_norm\": col_map[\"feature_norm\"],\n",
    "        \"Data Type\": clean.dtypes.astype(str).values\n",
    "    })\n",
    "\n",
    "# RI significance (robust)\n",
    "ri_sig_cols = {c.lower(): c for c in ri_sig.columns}\n",
    "if \"variable\" in ri_sig_cols:\n",
    "    var_col = ri_sig_cols[\"variable\"]\n",
    "else:\n",
    "    var_candidates = [c for c in ri_sig.columns if \"var\" in c.lower()]\n",
    "    var_col = var_candidates[0] if var_candidates else ri_sig.columns[0]\n",
    "ri_sig = ri_sig.copy()\n",
    "ri_sig[\"feature_norm\"] = ri_sig[var_col].map(norm)\n",
    "\n",
    "# Univariate summary\n",
    "uni_cols = {c.lower(): c for c in uni_top.columns}\n",
    "feat_col_uni = uni_cols.get(\"feature\", list(uni_top.columns)[0])\n",
    "p_col_uni = uni_cols.get(\"p-value\", None)\n",
    "\n",
    "eff_col_uni = None\n",
    "for k in [\"effect size\", \"effect_size\", \"effsize\"]:\n",
    "    if k in uni_cols:\n",
    "        eff_col_uni = uni_cols[k]\n",
    "        break\n",
    "\n",
    "uni_top = uni_top.copy()\n",
    "uni_top[\"feature_norm\"] = uni_top[feat_col_uni].map(norm)\n",
    "if p_col_uni and p_col_uni in uni_top.columns:\n",
    "    uni_top[\"p_value\"] = pd.to_numeric(uni_top[p_col_uni], errors=\"coerce\")\n",
    "else:\n",
    "    uni_top[\"p_value\"] = np.nan\n",
    "\n",
    "if eff_col_uni and eff_col_uni in uni_top.columns:\n",
    "    uni_top[\"effect_size\"] = pd.to_numeric(uni_top[eff_col_uni], errors=\"coerce\")\n",
    "else:\n",
    "    uni_top[\"effect_size\"] = np.nan\n",
    "\n",
    "# Coefficient tables\n",
    "coef_frailty_p = prep_coef(coef_frailty)\n",
    "coef_adr_p = prep_coef(coef_adr)\n",
    "coef_readm_p = prep_coef(coef_readm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c705cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize meta with all columns from cleaned data\n",
    "meta = pd.DataFrame({\"feature_norm\": col_map[\"feature_norm\"].unique()})\n",
    "meta = meta.merge(col_map.groupby(\"feature_norm\").agg(feature_orig=(\"feature_orig\",\"first\")).reset_index(),\n",
    "                  on=\"feature_norm\", how=\"left\")\n",
    "\n",
    "# Attach data type\n",
    "if \"Data Type\" in types.columns:\n",
    "    meta = meta.merge(types[[\"feature_norm\",\"Data Type\"]], on=\"feature_norm\", how=\"left\")\n",
    "else:\n",
    "    dtype_map = clean.dtypes.astype(str).to_dict()\n",
    "    meta[\"Data Type\"] = meta[\"feature_orig\"].map(dtype_map)\n",
    "\n",
    "# Attach RI metrics (when available)\n",
    "for col in [\"Combined_RI\",\"Mean_log_effect\",\"Mean_sd\",\"Mean_sel\"]:\n",
    "    src = [c for c in ri_sig.columns if c.lower()==col.lower()]\n",
    "    if src:\n",
    "        meta = meta.merge(ri_sig[[\"feature_norm\", src[0]]].rename(columns={src[0]: col}), on=\"feature_norm\", how=\"left\")\n",
    "\n",
    "# Univariate aggregation across outcomes\n",
    "agg_uni = (uni_top\n",
    "           .groupby(\"feature_norm\", as_index=False)\n",
    "           .agg(min_p_value=(\"p_value\",\"min\"),\n",
    "                max_abs_effect=(\"effect_size\", lambda s: np.nanmax(np.abs(s)))))\n",
    "meta = meta.merge(agg_uni, on=\"feature_norm\", how=\"left\")\n",
    "\n",
    "# Coefficient magnitudes (max abs per feature across families)\n",
    "def attach_coef(meta_df, coef_df, name):\n",
    "    agg = (coef_df.groupby(\"feature_norm\", as_index=False)\n",
    "           .agg(**{f\"{name}_abs_coef\": (\"coefficient\", lambda s: np.nanmax(np.abs(s)))}))\n",
    "    return meta_df.merge(agg, on=\"feature_norm\", how=\"left\")\n",
    "\n",
    "meta = attach_coef(meta, coef_frailty_p, \"frailty\")\n",
    "meta = attach_coef(meta, coef_adr_p, \"adr\")\n",
    "meta = attach_coef(meta, coef_readm_p, \"readm\")\n",
    "\n",
    "# Scores (percentile ranks)\n",
    "meta[\"score_ri\"] = rank01(meta[\"Combined_RI\"])\n",
    "meta[\"score_logeff\"] = rank01(meta[\"Mean_log_effect\"].abs())\n",
    "meta[\"score_p\"] = rank01(-np.log10(meta[\"min_p_value\"]))  # smaller p => larger score\n",
    "\n",
    "for nm in [\"frailty_abs_coef\",\"adr_abs_coef\",\"readm_abs_coef\",\"max_abs_effect\"]:\n",
    "    if nm in meta.columns:\n",
    "        meta[f\"score_{nm}\"] = rank01(meta[nm])\n",
    "\n",
    "# Weighted ensemble\n",
    "weights = {\n",
    "    \"score_ri\": 0.35,\n",
    "    \"score_logeff\": 0.15,\n",
    "    \"score_p\": 0.20,\n",
    "    \"score_frailty_abs_coef\": 0.10,\n",
    "    \"score_adr_abs_coef\": 0.10,\n",
    "    \"score_readm_abs_coef\": 0.10\n",
    "}\n",
    "\n",
    "def weighted_sum(row):\n",
    "    total, wsum = 0.0, 0.0\n",
    "    for k, w in weights.items():\n",
    "        if k in row.index and pd.notna(row[k]):\n",
    "            total += w * row[k]\n",
    "            wsum += w\n",
    "    return total / wsum if wsum > 0 else np.nan\n",
    "\n",
    "meta[\"feature_score\"] = meta.apply(weighted_sum, axis=1)\n",
    "meta.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca8f067",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Selection logic\n",
    "candidates = meta.sort_values(\"feature_score\", ascending=False)\n",
    "\n",
    "q75 = candidates[\"feature_score\"].quantile(0.75)\n",
    "sel_high = candidates[candidates[\"feature_score\"] >= q75].copy()\n",
    "sel_p = candidates[(candidates[\"min_p_value\"] <= 0.05)].copy()\n",
    "\n",
    "selected = pd.concat([sel_high, sel_p], axis=0).drop_duplicates(subset=[\"feature_norm\"])\n",
    "selected = selected.sort_values(\"feature_score\", ascending=False).head(TOP_N).copy()\n",
    "\n",
    "# Retain only columns that exist in the cleaned dataset\n",
    "selected = selected[selected[\"feature_orig\"].isin(clean.columns)]\n",
    "\n",
    "selected.shape, selected.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc70e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Try to locate a patient id column\n",
    "id_candidates = [c for c in clean.columns if re.search(r\"(patient.*id|id.*patient|^id$)\", c, flags=re.I)]\n",
    "id_col = id_candidates[0] if id_candidates else None\n",
    "\n",
    "keep_cols = ([id_col] if id_col else []) + selected[\"feature_orig\"].tolist()\n",
    "integrated = clean[keep_cols].copy()\n",
    "\n",
    "integrated.shape, id_col, integrated.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702c5aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "meta.to_csv(ART_DIR / \"feature_metadata_scores.csv\", index=False)\n",
    "selected.to_csv(ART_DIR / \"selected_features_manifest.csv\", index=False)\n",
    "integrated.to_csv(ART_DIR / \"phase3_integrated_data.csv\", index=False)\n",
    "\n",
    "print(\"Artifacts saved:\")\n",
    "print(\" -\", ART_DIR / \"feature_metadata_scores.csv\")\n",
    "print(\" -\", ART_DIR / \"selected_features_manifest.csv\")\n",
    "print(\" -\", ART_DIR / \"phase3_integrated_data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15265993",
   "metadata": {},
   "source": [
    "\n",
    "## Next steps (run in a new notebook or continue here)\n",
    "\n",
    "- Compute **Gower distance** on `phase3_integrated_data.csv`\n",
    "- Run **PCA** (numeric subset) and **UMAP** (metric='precomputed' on Gower)\n",
    "- Cluster with **k-prototypes** and **hierarchical** approaches\n",
    "- Validate with silhouette and bootstrap stability\n",
    "- Map clusters to survival, ADR, readmission, and frailty outcomes to label phenotypes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
