{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad260f0c",
   "metadata": {},
   "source": [
    "\n",
    "# Phase 3 - Oncogeriatrics Phenotyping (Clean Notebook)\n",
    "\n",
    "This notebook rebuilds the phenotyping pipeline from scratch with a clean, reproducible structure.\n",
    "\n",
    "**What this notebook does**\n",
    "- Loads the integrated cohort (CSV/Parquet) or generates a synthetic demo if the file is missing\n",
    "- Applies coherent preprocessing (type casting, imputation, scaling)\n",
    "- Performs dimensionality reduction (optional) and clustering\n",
    "- Evaluates internal validity (silhouette, DB, CH), cluster stability (bootstrap ARI), and clinical coherence (simple outcome enrichment)\n",
    "- Optionally fits survival curves per phenotype if `lifelines` is available\n",
    "- Exports artifacts (labels, summaries, figures) for the next phases\n",
    "\n",
    "> Tip: run top to bottom. If you have a real dataset, set `DATA_PATH` in the Config cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ee3a031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lifelines available: True\n",
      "OUTPUT_DIR: /mnt/data/phase3_outputs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =====================\n",
    "# Config and Imports\n",
    "# =====================\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Survival is optional\n",
    "try:\n",
    "    from lifelines import KaplanMeierFitter\n",
    "    LIFELINES_AVAILABLE = True\n",
    "except Exception:\n",
    "    LIFELINES_AVAILABLE = False\n",
    "\n",
    "# Plots - matplotlib only (no seaborn)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Clean output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# I/O\n",
    "DATA_PATH = \"\"  # e.g., \"/mnt/data/your_integrated_cohort.csv\" or \".parquet\"\n",
    "OUTPUT_DIR = \"/mnt/data/phase3_outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"lifelines available: {LIFELINES_AVAILABLE}\")\n",
    "print(f\"OUTPUT_DIR: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38c9553",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================\n",
    "# Data Loading\n",
    "# =====================\n",
    "def load_data(path: str = DATA_PATH) -> pd.DataFrame:\n",
    "    if path and os.path.exists(path):\n",
    "        ext = os.path.splitext(path)[1].lower()\n",
    "        if ext in [\".csv\"]:\n",
    "            df = pd.read_csv(path)\n",
    "        elif ext in [\".parquet\"]:\n",
    "            df = pd.read_parquet(path)\n",
    "        elif ext in [\".dta\"]:\n",
    "            df = pd.read_stata(path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file extension: {ext}\")\n",
    "        print(f\"Loaded real dataset: {path} with shape {df.shape}\")\n",
    "        return df\n",
    "    else:\n",
    "        # Synthetic fallback to keep notebook fully runnable\n",
    "        n = 600\n",
    "        df = pd.DataFrame({\n",
    "            \"age\": np.random.normal(74, 6, n).clip(55, 95),\n",
    "            \"bmi\": np.random.normal(26, 4.5, n).clip(16, 45),\n",
    "            \"hemoglobin\": np.random.normal(12.5, 1.5, n).clip(7, 18),\n",
    "            \"wbc\": np.random.normal(7.2, 2.0, n).clip(2, 20),\n",
    "            \"platelets\": np.random.normal(240, 60, n).clip(80, 600),\n",
    "            \"stage\": np.random.choice([\"I\",\"II\",\"III\",\"IV\"], n, p=[0.25,0.35,0.25,0.15]),\n",
    "            \"treatment_line\": np.random.choice([1,2,3], n, p=[0.6, 0.3, 0.1]),\n",
    "            \"surgery\": np.random.choice([0,1], n, p=[0.4,0.6]),\n",
    "            \"sex\": np.random.choice([\"F\",\"M\"], n),\n",
    "            # outcomes\n",
    "            \"overall_survival_event\": np.random.choice([0,1], n, p=[0.62,0.38]),\n",
    "            \"overall_survival_days\": np.random.exponential(700, size=n).astype(int) + np.random.randint(0,200,n),\n",
    "        })\n",
    "        # Inject some missingness\n",
    "        for col in [\"bmi\", \"hemoglobin\", \"wbc\", \"platelets\"]:\n",
    "            mask = np.random.rand(n) < 0.06\n",
    "            df.loc[mask, col] = np.nan\n",
    "        print(f\"Generated synthetic dataset with shape {df.shape}\")\n",
    "        return df\n",
    "\n",
    "df = load_data()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee081dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================\n",
    "# Feature Registry and Cohort Definition\n",
    "# =====================\n",
    "\n",
    "NUMERIC_FEATURES = [\n",
    "    \"age\",\"bmi\",\"hemoglobin\",\"wbc\",\"platelets\"\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURES = [\n",
    "    \"stage\",\"treatment_line\",\"surgery\",\"sex\"\n",
    "]\n",
    "\n",
    "OUTCOME_COLS = [\"overall_survival_event\", \"overall_survival_days\"]\n",
    "\n",
    "def define_cohort(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Example inclusion: non-missing age and stage\n",
    "    cohort = data.dropna(subset=[\"age\"]).copy()\n",
    "    # Example bounds\n",
    "    cohort = cohort[(cohort[\"age\"] >= 55) & (cohort[\"age\"] <= 95)]\n",
    "    print(f\"Cohort size after basic inclusion: {cohort.shape[0]}\")\n",
    "    return cohort\n",
    "\n",
    "cohort = define_cohort(df)\n",
    "cohort.sample(5, random_state=RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7c412d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================\n",
    "# Preprocessing Pipeline\n",
    "# =====================\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, NUMERIC_FEATURES),\n",
    "        (\"cat\", categorical_transformer, CATEGORICAL_FEATURES)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Optional PCA (set N_COMPONENTS to None to skip)\n",
    "N_COMPONENTS = 5\n",
    "\n",
    "def embed_features(X: pd.DataFrame, use_pca: bool = True) -> Tuple[np.ndarray, Dict[str, any]]:\n",
    "    Xt = preprocessor.fit_transform(X)\n",
    "    meta = {\"preprocessor\": preprocessor}\n",
    "    if use_pca and N_COMPONENTS is not None and N_COMPONENTS > 0:\n",
    "        pca = PCA(n_components=min(N_COMPONENTS, Xt.shape[1]), random_state=RANDOM_SEED)\n",
    "        Z = pca.fit_transform(Xt)\n",
    "        meta[\"pca\"] = pca\n",
    "        meta[\"embedding_name\"] = \"PCA\"\n",
    "        return Z, meta\n",
    "    else:\n",
    "        meta[\"embedding_name\"] = \"RawScaled\"\n",
    "        return Xt, meta\n",
    "\n",
    "X = cohort[NUMERIC_FEATURES + CATEGORICAL_FEATURES].copy()\n",
    "Z, embed_meta = embed_features(X, use_pca=True)\n",
    "print(embed_meta[\"embedding_name\"], Z.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c603e09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================\n",
    "# Clustering\n",
    "# =====================\n",
    "@dataclass\n",
    "class ClusterConfig:\n",
    "    algorithm: str = \"kmeans\"   # \"kmeans\", \"gmm\", \"agg\"\n",
    "    n_clusters: int = 3\n",
    "    random_state: int = RANDOM_SEED\n",
    "\n",
    "CONFIG = ClusterConfig(algorithm=\"kmeans\", n_clusters=4, random_state=RANDOM_SEED)\n",
    "\n",
    "def fit_cluster(Z: np.ndarray, cfg: ClusterConfig) -> Tuple[np.ndarray, object]:\n",
    "    if cfg.algorithm == \"kmeans\":\n",
    "        model = KMeans(n_clusters=cfg.n_clusters, n_init=10, random_state=cfg.random_state)\n",
    "        labels = model.fit_predict(Z)\n",
    "    elif cfg.algorithm == \"gmm\":\n",
    "        model = GaussianMixture(n_components=cfg.n_clusters, random_state=cfg.random_state)\n",
    "        labels = model.fit_predict(Z)\n",
    "    elif cfg.algorithm == \"agg\":\n",
    "        model = AgglomerativeClustering(n_clusters=cfg.n_clusters)\n",
    "        labels = model.fit_predict(Z)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown algorithm: {cfg.algorithm}\")\n",
    "    return labels, model\n",
    "\n",
    "labels, cluster_model = fit_cluster(Z, CONFIG)\n",
    "cohort = cohort.copy()\n",
    "cohort[\"phenotype\"] = labels.astype(int)\n",
    "cohort[\"phenotype\"].value_counts().sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b95af2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================\n",
    "# Internal Validity\n",
    "# =====================\n",
    "def internal_validity(z: np.ndarray, labels: np.ndarray) -> dict:\n",
    "    res = {}\n",
    "    # Some metrics require at least 2 clusters and no singletons edge-case\n",
    "    if len(np.unique(labels)) > 1 and z.shape[0] > len(np.unique(labels)):\n",
    "        res[\"silhouette\"] = float(silhouette_score(z, labels))\n",
    "        res[\"calinski_harabasz\"] = float(calinski_harabasz_score(z, labels))\n",
    "        res[\"davies_bouldin\"] = float(davies_bouldin_score(z, labels))\n",
    "    else:\n",
    "        res[\"silhouette\"] = np.nan\n",
    "        res[\"calinski_harabasz\"] = np.nan\n",
    "        res[\"davies_bouldin\"] = np.nan\n",
    "    return res\n",
    "\n",
    "iv = internal_validity(Z, labels)\n",
    "print(iv)\n",
    "\n",
    "# Simple 2D scatter if we used PCA >= 2\n",
    "if Z.shape[1] >= 2:\n",
    "    plt.figure(figsize=(6,5))\n",
    "    for k in np.unique(labels):\n",
    "        sel = labels == k\n",
    "        plt.scatter(Z[sel,0], Z[sel,1], s=10, alpha=0.7, label=f\"Phenotype {k}\")\n",
    "    plt.title(\"Embedding scatter by phenotype\")\n",
    "    plt.xlabel(\"PC1\" if \"pca\" in embed_meta else \"Dim 1\")\n",
    "    plt.ylabel(\"PC2\" if \"pca\" in embed_meta else \"Dim 2\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, \"embedding_scatter.png\"), dpi=150)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4297de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================\n",
    "# Stability (Bootstrap ARI)\n",
    "# =====================\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "\n",
    "def bootstrap_ari(z: np.ndarray, cfg: ClusterConfig, n_boot: int = 30, sample_frac: float = 0.8) -> float:\n",
    "    base_labels, _ = fit_cluster(z, cfg)\n",
    "    n = z.shape[0]\n",
    "    aris = []\n",
    "    for b in range(n_boot):\n",
    "        idx = np.random.choice(n, size=int(n*sample_frac), replace=False)\n",
    "        boot_labels, _ = fit_cluster(z[idx], cfg)\n",
    "        # To compute ARI, align to base on the subset only\n",
    "        aris.append(adjusted_rand_score(base_labels[idx], boot_labels))\n",
    "    return float(np.mean(aris))\n",
    "\n",
    "ari = bootstrap_ari(Z, CONFIG, n_boot=25, sample_frac=0.8)\n",
    "print({\"bootstrap_ARI\": ari})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea4a152",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================\n",
    "# Clinical Coherence\n",
    "# =====================\n",
    "def summarize_by_phenotype(df_: pd.DataFrame, numeric: list, categorical: list) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    num_summary = df_.groupby(\"phenotype\")[numeric].agg([\"count\",\"mean\",\"std\",\"median\",\"min\",\"max\"])\n",
    "    # flatten columns\n",
    "    num_summary.columns = [\"_\".join([c for c in col if c]) for col in num_summary.columns.to_flat_index()]\n",
    "    cat_summary = {}\n",
    "    for c in categorical:\n",
    "        cat_summary[c] = (df_.groupby([\"phenotype\", c]).size() / df_.groupby(\"phenotype\").size()).unstack(fill_value=0.0)\n",
    "    cat_summary = {k: v for k, v in cat_summary.items()}\n",
    "    return num_summary, cat_summary\n",
    "\n",
    "num_sum, cat_sums = summarize_by_phenotype(cohort, NUMERIC_FEATURES, CATEGORICAL_FEATURES)\n",
    "num_sum.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2272b26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================\n",
    "# Outcome Checks (and optional KM)\n",
    "# =====================\n",
    "# Chi-square for event rate differences and ANOVA for survival days (simple illustration)\n",
    "from scipy.stats import chi2_contingency, f_oneway\n",
    "\n",
    "# Event rates table by phenotype\n",
    "event_tab = pd.crosstab(cohort[\"phenotype\"], cohort[\"overall_survival_event\"])\n",
    "if event_tab.shape[1] == 2:\n",
    "    chi2, p, dof, exp = chi2_contingency(event_tab.values)\n",
    "    print({\"chi2_event_by_pheno\": chi2, \"p_value\": p})\n",
    "else:\n",
    "    print(\"Event table not binary. Skipping chi2.\")\n",
    "\n",
    "# Survival days ANOVA (crude; real analysis should use survival models)\n",
    "groups = [g[\"overall_survival_days\"].dropna().values for _, g in cohort.groupby(\"phenotype\")]\n",
    "if len(groups) > 1:\n",
    "    F, p = f_oneway(*groups)\n",
    "    print({\"anova_surv_days_by_pheno\": F, \"p_value\": p})\n",
    "\n",
    "# Kaplan-Meier per phenotype if lifelines is available\n",
    "if LIFELINES_AVAILABLE:\n",
    "    km = KaplanMeierFitter()\n",
    "    plt.figure(figsize=(7,5))\n",
    "    for k, g in cohort.groupby(\"phenotype\"):\n",
    "        try:\n",
    "            km.fit(durations=g[\"overall_survival_days\"], event_observed=g[\"overall_survival_event\"], label=f\"Pheno {k}\")\n",
    "            km.plot()\n",
    "        except Exception:\n",
    "            pass\n",
    "    plt.title(\"Kaplan-Meier curves by phenotype\")\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Survival probability\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, \"km_by_phenotype.png\"), dpi=150)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"lifelines not installed. Skipping KM plot.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e1301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================\n",
    "# Export Artifacts\n",
    "# =====================\n",
    "labels_path = os.path.join(OUTPUT_DIR, \"phenotype_labels.csv\")\n",
    "summary_path = os.path.join(OUTPUT_DIR, \"numeric_summary.csv\")\n",
    "\n",
    "cohort[[\"phenotype\"] + OUTCOME_COLS].to_csv(labels_path, index=False)\n",
    "num_sum.to_csv(summary_path)\n",
    "\n",
    "# Save categorical distributions\n",
    "for k, tab in cat_sums.items():\n",
    "    outp = os.path.join(OUTPUT_DIR, f\"cat_{k}_distribution.csv\")\n",
    "    tab.to_csv(outp)\n",
    "\n",
    "print({\n",
    "    \"labels_csv\": labels_path,\n",
    "    \"numeric_summary_csv\": summary_path,\n",
    "    \"figures_example\": [\n",
    "        os.path.join(OUTPUT_DIR, \"embedding_scatter.png\"),\n",
    "        os.path.join(OUTPUT_DIR, \"km_by_phenotype.png\")\n",
    "    ]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753e0021",
   "metadata": {},
   "source": [
    "\n",
    "## Next steps and how to plug your real data\n",
    "\n",
    "1. Set `DATA_PATH` in the Config cell to your integrated cohort file (CSV or Parquet recommended).\n",
    "2. Update `NUMERIC_FEATURES`, `CATEGORICAL_FEATURES`, and `OUTCOME_COLS` to match your schema.\n",
    "3. Adjust inclusion criteria in `define_cohort` to match clinical definitions for your study.\n",
    "4. Tune clustering in `ClusterConfig` (algorithm, number of clusters).\n",
    "5. Re-run the notebook top-to-bottom and review internal validity, stability, and clinical coherence outputs.\n",
    "6. The generated artifacts in `/mnt/data/phase3_outputs` can feed your translational and validation phases.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
